{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3W1_DataAugmentation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrueneKatze/GANs_Coursera/blob/main/C3W1_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bFRXfdmz5aN"
      },
      "source": [
        "#Data Augmentation\r\n",
        "##Goals\r\n",
        "In this notebook you're going to build a generator that can be used to help create data to train a classifier. There are many cases where this might be useful. If you are interested in any of these topics, you are welcome to explore the linked papers and articles!\r\n",
        "\r\n",
        "* With smaller datasets, GANs can provide useful data augmentation that substantially [improve classifier performance](https://arxiv.org/abs/1711.04340).\r\n",
        "* You have one type of data already labeled and would like to make predictions on [another related dataset for which you have no labels](https://www.nature.com/articles/s41598-019-52737-x). (You'll learn about the techniques for this use case in future notebooks!)\r\n",
        "* You want to protect the privacy of the people who provided their information so you can provide access to a [generator instead of real data](https://www.ahajournals.org/doi/full/10.1161/CIRCOUTCOMES.118.005122).\r\n",
        "* You have [input data with many missing values](https://arxiv.org/abs/1806.02920), where the input dimensions are correlated and you would like to train a model on complete inputs.\r\n",
        "* You would like to be able to identify a real-world abnormal feature in an image for the [purpose of diagnosis](https://link.springer.com/chapter/10.1007/978-3-030-00946-5_11), but have limited access to real examples of the condition.\r\n",
        "In this assignment, you're going to be acting as a bug enthusiast — more on that later.\r\n",
        "\r\n",
        "##Learning Objectives\r\n",
        "* Understand some use cases for data augmentation and why GANs suit this task.\r\n",
        "* Implement a classifier that takes a mixed dataset of reals/fakes and analyze its accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaL0UA97UNf"
      },
      "source": [
        "##Getting Started\r\n",
        "###Data Augmentation\r\n",
        "Before you implement GAN-based data augmentation, you should know a bit about data augmentation in general, specifically for image datasets. \r\n",
        "\r\n",
        "It is very [common practice](https://arxiv.org/abs/1712.04621) to augment image-based datasets in ways that are appropriate for a given dataset. This may include having your dataloader randomly flipping images across their vertical axis, randomly cropping your image to a particular size, randomly adding a bit of noise or color to an image in ways that are true-to-life.\r\n",
        "\r\n",
        "In general, data augmentation helps to stop your model from overfitting to the data, and allows you to make small datasets many times larger. However, a sufficiently powerful classifier often still overfits to the original examples which is why GANs are particularly useful here. They can generate new images instead of simply modifying existing ones.\r\n",
        "\r\n",
        "###CIFAR\r\n",
        "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) datasets are extremely widely used within machine learning -- they contain many thousands of “tiny” 32x32 color images of different classes representing relatively common real-world objects like airplanes and dogs, with 10 classes in CIFAR-10 and 100 classes in CIFAR-100. In CIFAR-100, there are 20 “superclasses” which each contain five classes. For example, the “fish” superclass contains “aquarium fish, flatfish, ray, shark, trout”. For the purposes of this assignment, you’ll be looking at a small subset of these images to simulate a small data regime, with only 40 images of each class for training.\r\n",
        "\r\n",
        "###Initializations\r\n",
        "You will begin by importing some useful libraries and packages and defining a visualization function that has been provided. You will also be re-using your conditional generator and functions code from earlier assignments. This will let you control what class of images to augment for your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7UR0lFR6qcI"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torch import nn\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!\r\n",
        "\r\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(3, 32, 32), nrow=5, show=True):\r\n",
        "    '''\r\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\r\n",
        "    size per image, plots and prints the images in an uniform grid.\r\n",
        "    '''\r\n",
        "    image_tensor = (image_tensor + 1) / 2\r\n",
        "    image_unflat = image_tensor.detach().cpu()\r\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\r\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "    if show:\r\n",
        "        plt.show()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-b5pnBB6qYu"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "    '''\r\n",
        "    Generator Class\r\n",
        "    Values:\r\n",
        "        input_dim: the dimension of the input vector, a scalar\r\n",
        "        im_chan: the number of channels of the output image, a scalar\r\n",
        "              (CIFAR100 is in color (red, green, blue), so 3 is your default)\r\n",
        "        hidden_dim: the inner dimension, a scalar\r\n",
        "    '''\r\n",
        "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "        self.input_dim = input_dim\r\n",
        "        # Build the neural network\r\n",
        "        self.gen = nn.Sequential(\r\n",
        "            self.make_gen_block(input_dim, hidden_dim * 4, kernel_size=4),\r\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\r\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim, kernel_size=4),\r\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=2, final_layer=True),\r\n",
        "        )\r\n",
        "\r\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\r\n",
        "        '''\r\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\r\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\r\n",
        "        Parameters:\r\n",
        "            input_channels: how many channels the input feature representation has\r\n",
        "            output_channels: how many channels the output feature representation should have\r\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\r\n",
        "            stride: the stride of the convolution\r\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \r\n",
        "                      (affects activation and batchnorm)\r\n",
        "        '''\r\n",
        "        if not final_layer:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "                nn.BatchNorm2d(output_channels),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "                nn.Tanh(),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, noise):\r\n",
        "        '''\r\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor, \r\n",
        "        returns generated images.\r\n",
        "        Parameters:\r\n",
        "            noise: a noise tensor with dimensions (n_samples, input_dim)\r\n",
        "        '''\r\n",
        "        x = noise.view(len(noise), self.input_dim, 1, 1)\r\n",
        "        return self.gen(x)\r\n",
        "\r\n",
        "\r\n",
        "def get_noise(n_samples, input_dim, device='cpu'):\r\n",
        "    '''\r\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\r\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\r\n",
        "    Parameters:\r\n",
        "        n_samples: the number of samples to generate, a scalar\r\n",
        "        input_dim: the dimension of the input vector, a scalar\r\n",
        "        device: the device type\r\n",
        "    '''\r\n",
        "    return torch.randn(n_samples, input_dim, device=device)\r\n",
        "\r\n",
        "def combine_vectors(x, y):\r\n",
        "    '''\r\n",
        "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?)\r\n",
        "    Parameters:\r\n",
        "    x: (n_samples, ?) the first vector. \r\n",
        "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \r\n",
        "        but you shouldn't need to know the second dimension's size.\r\n",
        "    y: (n_samples, ?) the second vector.\r\n",
        "        Once again, in this assignment this will be the one-hot class vector \r\n",
        "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\r\n",
        "    '''\r\n",
        "    return torch.cat([x, y], 1)\r\n",
        "\r\n",
        "def get_one_hot_labels(labels, n_classes):\r\n",
        "    '''\r\n",
        "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?)\r\n",
        "    Parameters:\r\n",
        "    labels: (n_samples, 1) \r\n",
        "    n_classes: a single integer corresponding to the total number of classes in the dataset\r\n",
        "    '''\r\n",
        "    return F.one_hot(labels, n_classes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QM6a5Cq_dbu"
      },
      "source": [
        "##Training\r\n",
        "Now you can begin training your models. First, you will define some new parameters:\r\n",
        "\r\n",
        "* cifar100_shape: the number of pixels in each CIFAR image, which has dimensions 32 x 32 and three channel (for red, green, and blue) so 3 x 32 x 32\r\n",
        "* n_classes: the number of classes in CIFAR100 (e.g. airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1eXv2kN6qVa"
      },
      "source": [
        "cifar100_shape = (3, 32, 32)\r\n",
        "n_classes = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6uKEwwD_o9p"
      },
      "source": [
        "And you also include the same parameters from previous assignments:\r\n",
        "\r\n",
        "* criterion: the loss function\r\n",
        "* n_epochs: the number of times you iterate through the entire dataset when training\r\n",
        "* z_dim: the dimension of the noise vector\r\n",
        "* display_step: how often to display/visualize the images\r\n",
        "* batch_size: the number of images per forward/backward pass\r\n",
        "* lr: the learning rate\r\n",
        "* device: the device type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXoICKwqBBLL"
      },
      "source": [
        "n_epochs = 10000\r\n",
        "z_dim = 64\r\n",
        "display_step = 500\r\n",
        "batch_size = 64\r\n",
        "lr = 0.0002\r\n",
        "device = 'cuda'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdEzr851BLFY"
      },
      "source": [
        "Then, you want to set your generator's input dimension. Recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNUz0QkXBPiu"
      },
      "source": [
        "generator_input_dim = z_dim + n_classes"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMscS6dBWE9"
      },
      "source": [
        "##Classifier\r\n",
        "For the classifier, you will use the same code that you wrote in an earlier assignment (the same as previous code for the discriminator as well since the discriminator is a real/fake classifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lqF4bxb6qR-"
      },
      "source": [
        "class Classifier(nn.Module):\r\n",
        "    '''\r\n",
        "    Classifier Class\r\n",
        "    Values:\r\n",
        "        im_chan: the number of channels of the output image, a scalar\r\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\r\n",
        "        hidden_dim: the inner dimension, a scalar\r\n",
        "    '''\r\n",
        "    def __init__(self, im_chan, n_classes, hidden_dim=32):\r\n",
        "        super(Classifier, self).__init__()\r\n",
        "        self.disc = nn.Sequential(\r\n",
        "            self.make_classifier_block(im_chan, hidden_dim),\r\n",
        "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\r\n",
        "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4),\r\n",
        "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\r\n",
        "        )\r\n",
        "\r\n",
        "    def make_classifier_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\r\n",
        "        '''\r\n",
        "        Function to return a sequence of operations corresponding to a classifier block; \r\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final\r\n",
        "        Parameters:\r\n",
        "            input_channels: how many channels the input feature representation has\r\n",
        "            output_channels: how many channels the output feature representation should have\r\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\r\n",
        "            stride: the stride of the convolution\r\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \r\n",
        "                      (affects activation and batchnorm)\r\n",
        "        '''\r\n",
        "        if not final_layer:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "                nn.BatchNorm2d(output_channels),\r\n",
        "                nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, image):\r\n",
        "        '''\r\n",
        "        Function for completing a forward pass of the classifier: Given an image tensor, \r\n",
        "        returns an n_classes-dimension tensor representing fake/real.\r\n",
        "        Parameters:\r\n",
        "            image: a flattened image tensor with im_chan channels\r\n",
        "        '''\r\n",
        "        class_pred = self.disc(image)\r\n",
        "        return class_pred.view(len(class_pred), -1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiiBC1t8COcv"
      },
      "source": [
        "###Pre-training (Optional)\r\n",
        "You are provided the code to pre-train the models (GAN and classifier) given to you in this assignment. However, this is intended only for your personal curiosity -- for the assignment to run as intended, you should not use any checkpoints besides the ones given to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiptUatY6qOm"
      },
      "source": [
        "# This code is here for you to train your own generator or classifier \r\n",
        "# outside the assignment on the full dataset if you'd like -- for the purposes \r\n",
        "# of this assignment, please use the provided checkpoints\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    '''\r\n",
        "    Discriminator Class\r\n",
        "    Values:\r\n",
        "      im_chan: the number of channels of the output image, a scalar\r\n",
        "            (MNIST is black-and-white, so 1 channel is your default)\r\n",
        "      hidden_dim: the inner dimension, a scalar\r\n",
        "    '''\r\n",
        "    def __init__(self, im_chan=3, hidden_dim=64):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "        self.disc = nn.Sequential(\r\n",
        "            self.make_disc_block(im_chan, hidden_dim, stride=1),\r\n",
        "            self.make_disc_block(hidden_dim, hidden_dim * 2),\r\n",
        "            self.make_disc_block(hidden_dim * 2, hidden_dim * 4),\r\n",
        "            self.make_disc_block(hidden_dim * 4, 1, final_layer=True),\r\n",
        "        )\r\n",
        "\r\n",
        "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\r\n",
        "        '''\r\n",
        "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \r\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\r\n",
        "        Parameters:\r\n",
        "            input_channels: how many channels the input feature representation has\r\n",
        "            output_channels: how many channels the output feature representation should have\r\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\r\n",
        "            stride: the stride of the convolution\r\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \r\n",
        "                      (affects activation and batchnorm)\r\n",
        "        '''\r\n",
        "        if not final_layer:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "                nn.BatchNorm2d(output_channels),\r\n",
        "                nn.LeakyReLU(0.2, inplace=True),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            return nn.Sequential(\r\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, image):\r\n",
        "        '''\r\n",
        "        Function for completing a forward pass of the discriminator: Given an image tensor, \r\n",
        "        returns a 1-dimension tensor representing fake/real.\r\n",
        "        Parameters:\r\n",
        "            image: a flattened image tensor with dimension (im_chan)\r\n",
        "        '''\r\n",
        "        disc_pred = self.disc(image)\r\n",
        "        return disc_pred.view(len(disc_pred), -1)\r\n",
        "\r\n",
        "def train_generator():\r\n",
        "    gen = Generator(generator_input_dim).to(device)\r\n",
        "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\r\n",
        "    discriminator_input_dim = cifar100_shape[0] + n_classes\r\n",
        "    disc = Discriminator(discriminator_input_dim).to(device)\r\n",
        "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\r\n",
        "\r\n",
        "    def weights_init(m):\r\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "        if isinstance(m, nn.BatchNorm2d):\r\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "            torch.nn.init.constant_(m.bias, 0)\r\n",
        "    gen = gen.apply(weights_init)\r\n",
        "    disc = disc.apply(weights_init)\r\n",
        "\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    cur_step = 0\r\n",
        "    mean_generator_loss = 0\r\n",
        "    mean_discriminator_loss = 0\r\n",
        "    for epoch in range(n_epochs):\r\n",
        "        # Dataloader returns the batches and the labels\r\n",
        "        for real, labels in dataloader:\r\n",
        "            cur_batch_size = len(real)\r\n",
        "            # Flatten the batch of real images from the dataset\r\n",
        "            real = real.to(device)\r\n",
        "\r\n",
        "            # Convert the labels from the dataloader into one-hot versions of those labels\r\n",
        "            one_hot_labels = get_one_hot_labels(labels.to(device), n_classes).float()\r\n",
        "\r\n",
        "            image_one_hot_labels = one_hot_labels[:, :, None, None]\r\n",
        "            image_one_hot_labels = image_one_hot_labels.repeat(1, 1, cifar100_shape[1], cifar100_shape[2])\r\n",
        "\r\n",
        "            ### Update discriminator ###\r\n",
        "            # Zero out the discriminator gradients\r\n",
        "            disc_opt.zero_grad()\r\n",
        "            # Get noise corresponding to the current batch_size \r\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\r\n",
        "        \r\n",
        "            # Combine the vectors of the noise and the one-hot labels for the generator\r\n",
        "            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\r\n",
        "            fake = gen(noise_and_labels)\r\n",
        "            # Combine the vectors of the images and the one-hot labels for the discriminator\r\n",
        "            fake_image_and_labels = combine_vectors(fake.detach(), image_one_hot_labels)\r\n",
        "            real_image_and_labels = combine_vectors(real, image_one_hot_labels)\r\n",
        "            disc_fake_pred = disc(fake_image_and_labels)\r\n",
        "            disc_real_pred = disc(real_image_and_labels)\r\n",
        "\r\n",
        "            disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\r\n",
        "            disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\r\n",
        "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\r\n",
        "            disc_loss.backward(retain_graph=True)\r\n",
        "            disc_opt.step() \r\n",
        "\r\n",
        "            # Keep track of the average discriminator loss\r\n",
        "            mean_discriminator_loss += disc_loss.item() / display_step\r\n",
        "\r\n",
        "            ### Update generator ###\r\n",
        "            # Zero out the generator gradients\r\n",
        "            gen_opt.zero_grad()\r\n",
        "\r\n",
        "            # Pass the discriminator the combination of the fake images and the one-hot labels\r\n",
        "            fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\r\n",
        "\r\n",
        "            disc_fake_pred = disc(fake_image_and_labels)\r\n",
        "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\r\n",
        "            gen_loss.backward()\r\n",
        "            gen_opt.step()\r\n",
        "\r\n",
        "            # Keep track of the average generator loss\r\n",
        "            mean_generator_loss += gen_loss.item() / display_step\r\n",
        "\r\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\r\n",
        "                print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\r\n",
        "                show_tensor_images(fake)\r\n",
        "                show_tensor_images(real)\r\n",
        "                mean_generator_loss = 0\r\n",
        "                mean_discriminator_loss = 0\r\n",
        "            cur_step += 1\r\n",
        "\r\n",
        "def train_classifier():\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    n_epochs = 10\r\n",
        "\r\n",
        "    validation_dataloader = DataLoader(\r\n",
        "        CIFAR100(\".\", train=False, download=True, transform=transform),\r\n",
        "        batch_size=batch_size)\r\n",
        "\r\n",
        "    display_step = 10\r\n",
        "    batch_size = 512\r\n",
        "    lr = 0.0002\r\n",
        "    device = 'cuda'\r\n",
        "    classifier = Classifier(cifar100_shape[0], n_classes).to(device)\r\n",
        "    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)\r\n",
        "    cur_step = 0\r\n",
        "    for epoch in range(n_epochs):\r\n",
        "        for real, labels in tqdm(dataloader):\r\n",
        "            cur_batch_size = len(real)\r\n",
        "            real = real.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "\r\n",
        "            ### Update classifier ###\r\n",
        "            # Get noise corresponding to the current batch_size\r\n",
        "            classifier_opt.zero_grad()\r\n",
        "            labels_hat = classifier(real.detach())\r\n",
        "            classifier_loss = criterion(labels_hat, labels)\r\n",
        "            classifier_loss.backward()\r\n",
        "            classifier_opt.step()\r\n",
        "\r\n",
        "            if cur_step % display_step == 0:\r\n",
        "                classifier_val_loss = 0\r\n",
        "                classifier_correct = 0\r\n",
        "                num_validation = 0\r\n",
        "                for val_example, val_label in validation_dataloader:\r\n",
        "                    cur_batch_size = len(val_example)\r\n",
        "                    num_validation += cur_batch_size\r\n",
        "                    val_example = val_example.to(device)\r\n",
        "                    val_label = val_label.to(device)\r\n",
        "                    labels_hat = classifier(val_example)\r\n",
        "                    classifier_val_loss += criterion(labels_hat, val_label) * cur_batch_size\r\n",
        "                    classifier_correct += (labels_hat.argmax(1) == val_label).float().sum()\r\n",
        "\r\n",
        "                print(f\"Step {cur_step}: \"\r\n",
        "                        f\"Classifier loss: {classifier_val_loss.item() / num_validation}, \"\r\n",
        "                        f\"classifier accuracy: {classifier_correct.item() / num_validation}\")\r\n",
        "            cur_step += 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bSuCvqxCcIY"
      },
      "source": [
        "##Tuning the Classifier\r\n",
        "After two courses, you've probably had some fun debugging your GANs and have started to consider yourself a bug master. For this assignment, your mastery will be put to the test on some interesting bugs... well, bugs as in insects.\r\n",
        "\r\n",
        "As a bug master, you want a classifier capable of classifying different species of bugs: bees, beetles, butterflies, caterpillar, and more. Luckily, you found a great dataset with a lot of animal species and objects, and you trained your classifier on that.\r\n",
        "\r\n",
        "But the bug classes don't do as well as you would like. Now your plan is to train a GAN on the same data so it can generate new bugs to make your classifier better at distinguishing between all of your favorite bugs!\r\n",
        "\r\n",
        "You will fine-tune your model by augmenting the original real data with fake data and during that process, observe how to increase the accuracy of your classifier with these fake, GAN-generated bugs. After this, you will prove your worth as a bug master.\r\n",
        "\r\n",
        "####Sampling Ratio\r\n",
        "Suppose that you've decided that although you have this pre-trained general generator and this general classifier, capable of identifying 100 classes with some accuracy (~17%), what you'd really like is a model that can classify the five different kinds of bugs in the dataset. You'll fine-tune your model by augmenting your data with the generated images. Keep in mind that both the generator and the classifier were trained on the same images: the 40 images per class you painstakingly found so your generator may not be great. This is the caveat with data augmentation, ultimately you are still bound by the real data that you have but you want to try and create more. To make your models even better, you would need to take some more bug photos, label them, and add them to your training set and/or use higher quality photos.\r\n",
        "\r\n",
        "To start, you'll first need to write some code to sample a combination of real and generated images. Given a probability, p_real, you'll need to generate a combined tensor where roughly p_real of the returned images are sampled from the real images. Note that you should not interpolate the images here: you should choose each image from the real or fake set with a given probability.\r\n",
        "\r\n",
        "In addition, we will expect the images to remain in the same order to maintain their alignment with their labels (this applies to the fake images too!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U3jcwY9j_mR"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n",
        "# GRADED FUNCTION: combine_sample\r\n",
        "def combine_sample(real, fake, p_real):\r\n",
        "    '''\r\n",
        "    Function to take a set of real and fake images of the same length (x)\r\n",
        "    and produce a combined tensor with length (x) and sampled at the target probability\r\n",
        "    Parameters:\r\n",
        "        real: a tensor of real images, length (x)\r\n",
        "        fake: a tensor of fake images, length (x)\r\n",
        "        p_real: the probability the images are sampled from the real set\r\n",
        "    '''\r\n",
        "    #### START CODE HERE ####\r\n",
        "    target_images = real.clone()\r\n",
        "    mask = torch.rand(len(real)) > (1-p_real)\r\n",
        "    target_images[mask==False] = fake[mask==False]\r\n",
        "    #### END CODE HERE ####\r\n",
        "    return target_images"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRZV8l68CVzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616fc234-a2fa-4cee-c853-15e1df55dbc5"
      },
      "source": [
        "n_test_samples = 9999\r\n",
        "test_combination = combine_sample(\r\n",
        "    torch.ones(n_test_samples, 1), \r\n",
        "    torch.zeros(n_test_samples, 1), \r\n",
        "    0.3\r\n",
        ")\r\n",
        "# Check that the shape is right\r\n",
        "assert tuple(test_combination.shape) == (n_test_samples, 1)\r\n",
        "# Check that the ratio is right\r\n",
        "assert torch.abs(test_combination.mean() - 0.3) < 0.05\r\n",
        "# Make sure that no mixing happened\r\n",
        "assert test_combination.median() < 1e-5\r\n",
        "\r\n",
        "test_combination = combine_sample(\r\n",
        "    torch.ones(n_test_samples, 10, 10), \r\n",
        "    torch.zeros(n_test_samples, 10, 10), \r\n",
        "    0.8\r\n",
        ")\r\n",
        "# Check that the shape is right\r\n",
        "assert tuple(test_combination.shape) == (n_test_samples, 10, 10)\r\n",
        "# Make sure that no mixing happened\r\n",
        "assert torch.abs((test_combination.sum([1, 2]).median()) - 100) < 1e-5\r\n",
        "\r\n",
        "test_reals = torch.arange(n_test_samples)[:, None].float()\r\n",
        "test_fakes = torch.zeros(n_test_samples, 1)\r\n",
        "test_saved = (test_reals.clone(), test_fakes.clone())\r\n",
        "test_combination = combine_sample(test_reals, test_fakes, 0.3)\r\n",
        "# Make sure that the sample isn't biased\r\n",
        "assert torch.abs((test_combination.mean() - 1500)) < 100\r\n",
        "# Make sure no inputs were changed\r\n",
        "assert torch.abs(test_saved[0] - test_reals).sum() < 1e-3\r\n",
        "assert torch.abs(test_saved[1] - test_fakes).sum() < 1e-3\r\n",
        "\r\n",
        "test_fakes = torch.arange(n_test_samples)[:, None].float()\r\n",
        "test_combination = combine_sample(test_reals, test_fakes, 0.3)\r\n",
        "# Make sure that the order is maintained\r\n",
        "assert torch.abs(test_combination - test_reals).sum() < 1e-4\r\n",
        "print(\"Success!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTjDGqfJCVvY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BrVABmlCVr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hFYRnt4CVn1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRxZh4lCVbH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFA24VZl6qAs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aBg4uS4wqDW"
      },
      "source": [
        "class ClassConditionalBatchNorm2d(nn.Module):\r\n",
        "    '''\r\n",
        "    ClassConditionalBatchNorm2d Class\r\n",
        "    Values:\r\n",
        "    in_channels: the dimension of the class embedding (c) + noise vector (z), a scalar\r\n",
        "    out_channels: the dimension of the activation tensor to be normalized, a scalar\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super().__init__()\r\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\r\n",
        "        self.class_scale_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\r\n",
        "        self.class_shift_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\r\n",
        "\r\n",
        "    def forward(self, x, y):\r\n",
        "        normalized_image = self.bn(x)\r\n",
        "        class_scale = (1 + self.class_scale_transform(y))[:, :, None, None]\r\n",
        "        class_shift = self.class_shift_transform(y)[:, :, None, None]\r\n",
        "        transformed_image = class_scale * normalized_image + class_shift\r\n",
        "        return transformed_image\r\n",
        "\r\n",
        "# class AdaIN(nn.Module):\r\n",
        "#     '''\r\n",
        "#     AdaIN Class, extends/subclass of nn.Module\r\n",
        "#     Values:\r\n",
        "#       channels: the number of channels the image has, a scalar\r\n",
        "#       w_dim: the dimension of the intermediate tensor, w, a scalar \r\n",
        "#     '''\r\n",
        "\r\n",
        "#     def __init__(self, channels, w_dim):\r\n",
        "#         super().__init__()\r\n",
        "#         self.instance_norm = nn.InstanceNorm2d(channels)\r\n",
        "#         self.style_scale_transform = nn.Linear(w_dim, channels)\r\n",
        "#         self.style_shift_transform = nn.Linear(w_dim, channels)\r\n",
        "\r\n",
        "#     def forward(self, image, w):\r\n",
        "#         normalized_image = self.instance_norm(image)\r\n",
        "#         style_scale = self.style_scale_transform(w)[:, :, None, None]\r\n",
        "#         style_shift = self.style_shift_transform(w)[:, :, None, None]\r\n",
        "#         transformed_image = style_scale * normalized_image + style_shift\r\n",
        "#         return transformed_image\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kULKAgQ5Qix"
      },
      "source": [
        "##Self-Attention Block\r\n",
        "As you may already know, self-attention has been a successful technique in helping models learn arbitrary, long-term dependencies. [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318) (Zhang et al. 2018) first introduced the self-attention mechanism into the GAN architecture. BigGAN augments its residual blocks with these attention blocks.\r\n",
        "\r\n",
        "###A Quick Primer on Self-Attention\r\n",
        "\r\n",
        "Self-attention is just scaled dot product attention. Given a sequence  𝑆  (with images,  𝑆  is just the image flattened across its height and width), the model learns mappings to query ( 𝑄 ), key ( 𝐾 ), and value ( 𝑉 ) matrices:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzSNebdB5Yel"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlU5opY-5hsw"
      },
      "source": [
        "where  𝑊𝑞 ,  𝑊𝑘 , and  𝑊𝑣  are learned parameters. The subsequent self-attention mechanism is then computed as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IHbqJIR5hha"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HIkKIsz5hXU"
      },
      "source": [
        "where  𝑑𝑘  is the dimensionality of the  𝑄,𝐾  matrices (SA-GAN and BigGAN both omit this term). Intuitively, you can think of the query matrix as containing the representations of each position with respect to itself and the key matrix as containing the representations of each position with respect to the others. How important two positions are to each other is measured by dot product as  𝑄𝐾⊤ , hence dot product attention. A softmax is applied to convert these relative importances to a probability distribution over all positions.\r\n",
        "\r\n",
        "Intuitively, the value matrix provides the importance weighting of the attention at each position, hence scaled dot product attention. Relevant positions should be assigned larger weight and irrelevant ones should be assigned smaller weight.\r\n",
        "\r\n",
        "Don't worry if you don't understand this right away - it's a tough concept! For extra reading, you should check out [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al. 2017), which is the paper that first introduces this technique, and [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), which breaks down and explains the self-attention mechanism clearly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA_jFhTpwp_u"
      },
      "source": [
        "class AttentionBlock(nn.Module):\r\n",
        "    '''\r\n",
        "    AttentionBlock Class\r\n",
        "    Values:\r\n",
        "    channels: number of channels in input\r\n",
        "    '''\r\n",
        "    def __init__(self, channels):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.channels = channels\r\n",
        "\r\n",
        "        self.theta = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\r\n",
        "        self.phi = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\r\n",
        "        self.g = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 2, kernel_size=1, padding=0, bias=False))\r\n",
        "        self.o = nn.utils.spectral_norm(nn.Conv2d(channels // 2, channels, kernel_size=1, padding=0, bias=False))\r\n",
        "\r\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        spatial_size = x.shape[2] * x.shape[3]\r\n",
        "\r\n",
        "        # Apply convolutions to get query (theta), key (phi), and value (g) transforms\r\n",
        "        theta = self.theta(x)\r\n",
        "        phi = F.max_pool2d(self.phi(x), kernel_size=2)\r\n",
        "        g = F.max_pool2d(self.g(x), kernel_size=2)\r\n",
        "\r\n",
        "        # Reshape spatial size for self-attention\r\n",
        "        theta = theta.view(-1, self.channels // 8, spatial_size)\r\n",
        "        phi = phi.view(-1, self.channels // 8, spatial_size // 4)\r\n",
        "        g = g.view(-1, self.channels // 2, spatial_size // 4)\r\n",
        "\r\n",
        "        # Compute dot product attention with query (theta) and key (phi) matrices\r\n",
        "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), dim=-1)\r\n",
        "\r\n",
        "        # Compute scaled dot product attention with value (g) and attention (beta) matrices\r\n",
        "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.channels // 2, x.shape[2], x.shape[3]))\r\n",
        "\r\n",
        "        # Apply gain and residual\r\n",
        "        return self.gamma * o + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPASByH29aJp"
      },
      "source": [
        "##BigGAN Generator\r\n",
        "Before implementing the generator in full, you first need to implement the generator residual block.\r\n",
        "\r\n",
        "###Generator Residual Block\r\n",
        "As with many state-of-the-art computer vision models, BigGAN employs skip connections in the form of residual blocks to map random noise to a fake image. You can think of BigGAN residual blocks as having 3 steps. Given input  𝑥  and class embedding  𝑦 :\r\n",
        "\r\n",
        "* ℎ:=  bn-relu-upsample-conv (𝑥,𝑦) \r\n",
        "* ℎ:=  bn-relu-conv (ℎ,𝑦) \r\n",
        "* 𝑥:=  upsample-conv (𝑥) ,\r\n",
        "\r\n",
        "after which you can apply a residual connection and return  ℎ+𝑥 .\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krzkl6B9wp7x"
      },
      "source": [
        "class GResidualBlock(nn.Module):\r\n",
        "    '''\r\n",
        "    GResidualBlock Class\r\n",
        "    Values:\r\n",
        "    c_dim: the dimension of conditional vector [c, z], a scalar\r\n",
        "    in_channels: the number of channels in the input, a scalar\r\n",
        "    out_channels: the number of channels in the output, a scalar\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, c_dim, in_channels, out_channels):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\r\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\r\n",
        "\r\n",
        "        self.bn1 = ClassConditionalBatchNorm2d(c_dim, in_channels)\r\n",
        "        self.bn2 = ClassConditionalBatchNorm2d(c_dim, out_channels)\r\n",
        "\r\n",
        "        self.activation = nn.ReLU()\r\n",
        "        self.upsample_fn = nn.Upsample(scale_factor=2)     # upsample occurs in every gblock\r\n",
        "\r\n",
        "        self.mixin = (in_channels != out_channels)\r\n",
        "        if self.mixin:\r\n",
        "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\r\n",
        "\r\n",
        "    def forward(self, x, y):\r\n",
        "        # h := upsample(x, y)\r\n",
        "        h = self.bn1(x, y)\r\n",
        "        h = self.activation(h)\r\n",
        "        h = self.upsample_fn(h)\r\n",
        "        h = self.conv1(h)\r\n",
        "\r\n",
        "        # h := conv(h, y)\r\n",
        "        h = self.bn2(h, y)\r\n",
        "        h = self.activation(h)\r\n",
        "        h = self.conv2(h)\r\n",
        "\r\n",
        "        # x := upsample(x)\r\n",
        "        x = self.upsample_fn(x)\r\n",
        "        if self.mixin:\r\n",
        "            x = self.conv_mixin(x)\r\n",
        "\r\n",
        "        return h + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNoNHAxg_l9Y"
      },
      "source": [
        "You can now implement the BigGAN generator in full!! Below is an implementation of the base model (at 128x128 resolution) from the paper.\r\n",
        "\r\n",
        "This implementation uses nn.ModuleList for convenience. If you're not familiar with this, you can think of it as simply a Pythonic list that registers your modules with the Pytorch backend. For more information, see the torch.nn.ModuleList documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG8V2l3D-jwC"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "    '''\r\n",
        "    Generator Class\r\n",
        "    Values:\r\n",
        "    z_dim: the dimension of random noise sampled, a scalar\r\n",
        "    shared_dim: the dimension of shared class embeddings, a scalar\r\n",
        "    base_channels: the number of base channels, a scalar\r\n",
        "    bottom_width: the height/width of image before it gets upsampled, a scalar\r\n",
        "    n_classes: the number of image classes, a scalar\r\n",
        "    '''\r\n",
        " \r\n",
        "    def __init__(self, base_channels=96, bottom_width=4, z_dim=120, shared_dim=128, n_classes=1000):\r\n",
        "        super().__init__()\r\n",
        " \r\n",
        "        n_chunks = 6    # 5 (generator blocks) + 1 (generator input)\r\n",
        "        self.z_chunk_size = z_dim // n_chunks\r\n",
        "        self.z_dim = z_dim\r\n",
        "        self.shared_dim = shared_dim\r\n",
        "        self.bottom_width = bottom_width\r\n",
        " \r\n",
        "        # No spectral normalization on embeddings, which authors observe to cripple the generator\r\n",
        "        self.shared_emb = nn.Embedding(n_classes, shared_dim)\r\n",
        " \r\n",
        "        self.proj_z = nn.Linear(self.z_chunk_size, 16 * base_channels * bottom_width ** 2)\r\n",
        " \r\n",
        "        # Can't use one big nn.Sequential since we are adding class+noise at each block\r\n",
        "        self.g_blocks = nn.ModuleList([\r\n",
        "            nn.ModuleList([\r\n",
        "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 16 * base_channels),\r\n",
        "                AttentionBlock(16 * base_channels),\r\n",
        "            ]),\r\n",
        "            nn.ModuleList([\r\n",
        "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 8 * base_channels),\r\n",
        "                AttentionBlock(8 * base_channels),\r\n",
        "            ]),\r\n",
        "            nn.ModuleList([\r\n",
        "                GResidualBlock(shared_dim + self.z_chunk_size, 8 * base_channels, 4 * base_channels),\r\n",
        "                AttentionBlock(4 * base_channels),\r\n",
        "            ]),\r\n",
        "            nn.ModuleList([\r\n",
        "                GResidualBlock(shared_dim + self.z_chunk_size, 4 * base_channels, 2 * base_channels),\r\n",
        "                AttentionBlock(2 * base_channels),\r\n",
        "            ]),\r\n",
        "            nn.ModuleList([\r\n",
        "                GResidualBlock(shared_dim + self.z_chunk_size, 2 * base_channels, base_channels),\r\n",
        "                AttentionBlock(base_channels),\r\n",
        "            ]),\r\n",
        "        ])\r\n",
        "        self.proj_o = nn.Sequential(\r\n",
        "            nn.BatchNorm2d(base_channels),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base_channels, 3, kernel_size=1, padding=0)),\r\n",
        "            nn.Tanh(),\r\n",
        "        )\r\n",
        " \r\n",
        "    def forward(self, z, y):\r\n",
        "        '''\r\n",
        "        z: random noise with size self.z_dim\r\n",
        "        y: class embeddings with size self.shared_dim\r\n",
        "            = NOTE =\r\n",
        "            y should be class embeddings from self.shared_emb, not the raw class labels\r\n",
        "        '''\r\n",
        "        # Chunk z and concatenate to shared class embeddings\r\n",
        "        zs = torch.split(z, self.z_chunk_size, dim=1)\r\n",
        "        z = zs[0]\r\n",
        "        ys = [torch.cat([y, z], dim=1) for z in zs[1:]]\r\n",
        " \r\n",
        "        # Project noise and reshape to feed through generator blocks\r\n",
        "        h = self.proj_z(z)\r\n",
        "        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)\r\n",
        " \r\n",
        "        # Feed through generator blocks\r\n",
        "        for idx, g_block in enumerate(self.g_blocks):\r\n",
        "            h = g_block[0](h, ys[idx])\r\n",
        "            h = g_block[1](h)\r\n",
        " \r\n",
        "        # Project to 3 RGB channels with tanh to map values to [-1, 1]\r\n",
        "        h = self.proj_o(h)\r\n",
        " \r\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFQK5yeBY0f"
      },
      "source": [
        "##BigGAN Discriminator\r\n",
        "Before implementing the discriminator in full, you need to implement a discriminator residual block, which is simpler than the generator's. Note that the last residual block does not apply downsampling.\r\n",
        "\r\n",
        "* ℎ:=  relu-conv-relu-downsample (𝑥) \r\n",
        "* 𝑥:=  conv-downsample (𝑥) \r\n",
        "\r\n",
        "In the official BigGAN implementation, the architecture is slightly different for the first discriminator residual block, since it handles the raw image as input:\r\n",
        "\r\n",
        "* ℎ:=  conv-relu-downsample (𝑥) \r\n",
        "* 𝑥:=  downsample-conv (𝑥) \r\n",
        "\r\n",
        "After these two steps, you can return the residual connection  ℎ+𝑥 . You might notice that there is no class information in these residual blocks. As you'll see later in the code, the authors inject class-conditional information after the final hidden layer (and before the output layer) via channel-wise dot product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mowi8ohx-jr2"
      },
      "source": [
        "class DResidualBlock(nn.Module):\r\n",
        "    '''\r\n",
        "    DResidualBlock Class\r\n",
        "    Values:\r\n",
        "    in_channels: the number of channels in the input, a scalar\r\n",
        "    out_channels: the number of channels in the output, a scalar\r\n",
        "    downsample: whether to apply downsampling\r\n",
        "    use_preactivation: whether to apply an activation function before the first convolution\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels, downsample=True, use_preactivation=False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\r\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\r\n",
        "\r\n",
        "        self.activation = nn.ReLU()\r\n",
        "        self.use_preactivation = use_preactivation  # apply preactivation in all except first dblock\r\n",
        "\r\n",
        "        self.downsample = downsample    # downsample occurs in all except last dblock\r\n",
        "        if downsample:\r\n",
        "            self.downsample_fn = nn.AvgPool2d(2)\r\n",
        "        self.mixin = (in_channels != out_channels) or downsample\r\n",
        "        if self.mixin:\r\n",
        "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\r\n",
        "\r\n",
        "    def _residual(self, x):\r\n",
        "        if self.use_preactivation:\r\n",
        "            if self.mixin:\r\n",
        "                x = self.conv_mixin(x)\r\n",
        "            if self.downsample:\r\n",
        "                x = self.downsample_fn(x)\r\n",
        "        else:\r\n",
        "            if self.downsample:\r\n",
        "                x = self.downsample_fn(x)\r\n",
        "            if self.mixin:\r\n",
        "                x = self.conv_mixin(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # Apply preactivation if applicable\r\n",
        "        if self.use_preactivation:\r\n",
        "            h = F.relu(x)\r\n",
        "        else:\r\n",
        "            h = x\r\n",
        "\r\n",
        "        h = self.conv1(h)\r\n",
        "        h = self.activation(h)\r\n",
        "        if self.downsample:\r\n",
        "            h = self.downsample_fn(h)\r\n",
        "\r\n",
        "        return h + self._residual(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4C_P9ndCACV"
      },
      "source": [
        "Now implement the BigGAN discriminator in full!!\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_0pKrZLCJho"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "    '''\r\n",
        "    Discriminator Class\r\n",
        "    Values:\r\n",
        "    base_channels: the number of base channels, a scalar\r\n",
        "    n_classes: the number of image classes, a scalar\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, base_channels=96, n_classes=1000):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # For adding class-conditional evidence\r\n",
        "        self.shared_emb = nn.utils.spectral_norm(nn.Embedding(n_classes, 16 * base_channels))\r\n",
        "\r\n",
        "        self.d_blocks = nn.Sequential(\r\n",
        "            DResidualBlock(3, base_channels, downsample=True, use_preactivation=False),\r\n",
        "            AttentionBlock(base_channels),\r\n",
        "\r\n",
        "            DResidualBlock(base_channels, 2 * base_channels, downsample=True, use_preactivation=True),\r\n",
        "            AttentionBlock(2 * base_channels),\r\n",
        "\r\n",
        "            DResidualBlock(2 * base_channels, 4 * base_channels, downsample=True, use_preactivation=True),\r\n",
        "            AttentionBlock(4 * base_channels),\r\n",
        "\r\n",
        "            DResidualBlock(4 * base_channels, 8 * base_channels, downsample=True, use_preactivation=True),\r\n",
        "            AttentionBlock(8 * base_channels),\r\n",
        "\r\n",
        "            DResidualBlock(8 * base_channels, 16 * base_channels, downsample=True, use_preactivation=True),\r\n",
        "            AttentionBlock(16 * base_channels),\r\n",
        "\r\n",
        "            DResidualBlock(16 * base_channels, 16 * base_channels, downsample=False, use_preactivation=True),\r\n",
        "            AttentionBlock(16 * base_channels),\r\n",
        "\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "        )\r\n",
        "        self.proj_o = nn.utils.spectral_norm(nn.Linear(16 * base_channels, 1))\r\n",
        "\r\n",
        "    def forward(self, x, y=None):\r\n",
        "        h = self.d_blocks(x)\r\n",
        "        h = torch.sum(h, dim=[2, 3])\r\n",
        "\r\n",
        "        # Class-unconditional output\r\n",
        "        uncond_out = self.proj_o(h)\r\n",
        "        if y is None:\r\n",
        "            return uncond_out\r\n",
        "\r\n",
        "        # Class-conditional output\r\n",
        "        cond_out = torch.sum(self.shared_emb(y) * h, dim=1, keepdim=True)\r\n",
        "        return uncond_out + cond_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XvAu606CW7N"
      },
      "source": [
        "##Setting Up BigGAN Training\r\n",
        "Now you're are ready to set up BigGAN for training! Unfortunately, this notebook will not provide actual training code due to the size of BigGAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrtLPJH-jm6"
      },
      "source": [
        "device = 'cpu'\r\n",
        "\r\n",
        "# Initialize models\r\n",
        "base_channels = 96\r\n",
        "z_dim = 120\r\n",
        "n_classes = 5   # 5 classes is used instead of the original 1000, for efficiency\r\n",
        "shared_dim = 128\r\n",
        "generator = Generator(base_channels=base_channels, bottom_width=4, z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\r\n",
        "discriminator = Discriminator(base_channels=base_channels, n_classes=n_classes).to(device)\r\n",
        "\r\n",
        "# Initialize weights orthogonally\r\n",
        "for module in generator.modules():\r\n",
        "    if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\r\n",
        "        nn.init.orthogonal_(module.weight)\r\n",
        "for module in discriminator.modules():\r\n",
        "    if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\r\n",
        "        nn.init.orthogonal_(module.weight)\r\n",
        "\r\n",
        "# Initialize optimizers\r\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999), eps=1e-6)\r\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.0, 0.999), eps=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmrLHhT5CjDq"
      },
      "source": [
        "Here is a sample forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNdHuh1G-jb5"
      },
      "source": [
        "batch_size = n_classes\r\n",
        "\r\n",
        "z = torch.randn(batch_size, z_dim, device=device)                 # Generate random noise (z)\r\n",
        "y = torch.arange(start=0, end=n_classes, device=device).long()    # Generate a batch of labels (y), one for each class\r\n",
        "y_emb = generator.shared_emb(y)                                   # Retrieve class embeddings (y_emb) from generator\r\n",
        "\r\n",
        "x_gen = generator(z, y_emb)                                       # Generate fake images from z and y_emb\r\n",
        "score = discriminator(x_gen, y)                                   # Generate classification for fake images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8B58WoICoWI"
      },
      "source": [
        "##BigGAN-deep\r\n",
        "Initially, the authors of the BigGAN paper didn't find much help in increasing the depth of the network. But they experimented further (research is always improving!) and added a few notes about an additional architecture, called BigGAN-deep. This modification of BigGAN is 4x deeper, sports a modified residual block architecture, and concatenates the entire  𝑧  vector to  𝑐  (as opposed to separate chunks at different resolutions).\r\n",
        "\r\n",
        "Typically on a difficult and complex task that you're unlikely to overfit, you expect better performance when a model has more parameters, because it has more room to learn. Surprisingly, BigGAN-deep has fewer parameters than its BigGAN counterpart. Architectural optimizations such as using depthwise separable convolutions and truncating/concatenating channels in skip connections (as opposed to using pointwise convolutions) decrease parameters without trading expressivity.\r\n",
        "\r\n",
        "For more details on the BigGAN-deep architecture, see Appendix B of the paper.\r\n",
        "\r\n",
        "And as for the implementation of the BigGAN-deep variant, well, that's left as an exercise for the reader. You're a smart cookie, you'll figure it out! Just keep in mind that with great power comes great responsibility ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd4FUXqNwpvo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erk4Zjv7zSzK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}